% To find publishing date of a website try this: (https://academia.stackexchange.com/questions/73612/citation-of-a-website-how-to-determine-the-year)
% run this:
% javascript:void((function(){var toRm=document.getElementById('showTagsWithDate');if(toRm){document.body.removeChild(toRm);return;}document.body.insertAdjacentHTML('afterbegin','<div id="showTagsWithDate" style="background-color:white;color:black;">Tags with a date in YYYY-[M]M-[D]D format; or in English (US, or non-US format):<ul/></div>');var myul=document.body.firstChild.lastChild;var tags=[];function addMoreDates(reg){var addTags=document.documentElement.innerHTML.match(reg);if(addTags){addTags.forEach(function(newTag){if((newTag.indexOf('<a ')===0)||(newTag.indexOf('<img '))===0){return;}if(tags.indexOf(newTag) === -1){tags.push(newTag);}});}}addMoreDates(/<[A-Z][^>]*\D(20\d\d|1\d\d\d)[\s\/\-.,]\s*([1-9]|0[1-9]|[1][012])[\s\/\-,.]\s*([1-9]|0[1-9]|[12]\d|3[01])\s*(st|nd|rd|th){0,1}\D[^>]*>/img);addMoreDates(/<[A-Z][^>]*\b([1-9]|0[1-9]|[12]\d|3[01])(st|nd|rd|th){0,1}[\/\-\s]\s*(january|february|march|april|may|june|july|august|september|october|november|december|jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)[\s,.\/\-][\s,.\/\-]?\s*(20\d\d|1\d\d\d)\b[^>]*>/img);addMoreDates(/<[A-Z][^>]*\b(january|february|march|april|may|june|july|august|september|october|november|december|jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)[\s,.\/\-][\s,.\/\-]?\s*([1-9]|0[1-9]|[12]\d|3[01])(st|nd|rd|th){0,1}[\s,.\-]+(20\d\d|1\d\d\d)\b[^>]*>/img);if(tags.length===0){tags=['No tags with dates.'];}tags.forEach(function(tag){myul.appendChild(document.createElement('LI')).appendChild(document.createTextNode(tag));});document.body.firstChild.appendChild(document.createElement('BR'));})())
% if it fails you may find the earliest appearence of the site in: https://archive.org/
% otherwise find the latest modification change or put the current year

% To save an archive of the site you can use this:
% javascript:void(window.open('https://web.archive.org/save/'+location.href))

@online{iscac,
    author = {ISCAC},
    title = {Síntese Histórica, 100 anos de Ciências Empresariais.},
    url = {http://www.iscac.pt/index.php?m=47_10&lang=PT},
    urldate = {2023-10-23},
    year = "2013",
    note = {\url{https://web.archive.org/web/20231024164219/http://www.iscac.pt/index.php?m=47_10&lang=PT}},
}

@inproceedings{wohlin_guidelines_2014,
	address = {New York, NY, USA},
	series = {{EASE} '14},
	title = {Guidelines for snowballing in systematic literature studies and a replication in software engineering},
	isbn = {978-1-4503-2476-2},
	url = {https://dl.acm.org/doi/10.1145/2601248.2601268},
	doi = {10.1145/2601248.2601268},
	abstract = {Background: Systematic literature studies have become common in software engineering, and hence it is important to understand how to conduct them efficiently and reliably.Objective: This paper presents guidelines for conducting literature reviews using a snowballing approach, and they are illustrated and evaluated by replicating a published systematic literature review.Method: The guidelines are based on the experience from conducting several systematic literature reviews and experimenting with different approaches.Results: The guidelines for using snowballing as a way to search for relevant literature was successfully applied to a systematic literature review.Conclusions: It is concluded that using snowballing, as a first search strategy, may very well be a good alternative to the use of database searches.},
	urldate = {2025-11-16},
	booktitle = {Proceedings of the 18th {International} {Conference} on {Evaluation} and {Assessment} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wohlin, Claes},
	month = may,
	year = {2014},
	pages = {1--10},
}

@article{webster_analyzing_2002,
	title = {Analyzing the {Past} to {Prepare} for the {Future}: {Writing} a {Literature} {Review}},
	volume = {26},
	issn = {0276-7783},
	shorttitle = {Analyzing the {Past} to {Prepare} for the {Future}},
	url = {https://www.jstor.org/stable/4132319},
	number = {2},
	urldate = {2025-09-25},
	journal = {MIS Quarterly},
	author = {Webster, Jane and Watson, Richard T.},
	year = {2002},
	note = {Publisher: Management Information Systems Research Center, University of Minnesota},
	pages = {xiii--xxiii},
}

@misc{AISafetyReport2025,
    author      = {{Department for Science, Innovation and Technology (DSIT)}},
    title       = {{International AI Safety Report 2025}},
    year        = {2025},
    month       = {January},
    howpublished = {Report published by the UK Government},
    url         = {https://www.gov.uk/government/publications/international-ai-safety-report-2025},
    urldate     = {2025-01-29}
}

@misc{FLIAISafetyIndex2025,
    author      = {{Future of Life Institute (FLI)}},
    title       = {{AI Safety Index – Summer 2025 Edition}},
    year        = {2025},
    month       = {July},
    note        = {A detailed, scored assessment of seven frontier AI developers' safety practices. Available online.},
    howpublished = {Report},
    url         = {https://futureoflife.org/ai-safety-index-summer-2025/},
    urldate     = {2025-07-14}
}

@article{yampolskiy_controllability_2022,
	title = {On the Controllability of Artificial Intelligence: An Analysis of Limitations},
	volume = {11},
	issn = {2245-1439},
	doi = {10.13052/jcsm2245-1439.1132},
	shorttitle = {On the Controllability of Artificial Intelligence},
	abstract = {The invention of artificial general intelligence is predicted to cause a shift in the trajectory of human civilization. In order to reap the benefits and avoid the pitfalls of such a powerful technology it is important to be able to control it. However, the possibility of controlling artificial general intelligence and its more advanced version, superintelligence, has not been formally established. In this paper, we present arguments as well as supporting evidence from multiple domains indicating that advanced {AI} cannot be fully controlled. The consequences of uncontrollability of {AI} are discussed with respect to the future of humanity and research on {AI}, and {AI} safety and security. © 2022 River Publishers.},
	pages = {321--404},
	number = {3},
	journaltitle = {Journal of Cyber Security and Mobility},
	shortjournal = {J. Cyber Secur. Mobil.},
	author = {Yampolskiy, Roman V.},
	date = {2022},
	note = {Publisher: River Publishers},
	keywords = {{AI} safety, control problem, safer {AI}, uncontrollability, unverifiability, X-risk, {ARTIFICIAL} intelligence, {ANOMALY} detection (Computer security), {CHEMICAL} processes, Computer Systems Design Services, {HAZARD} mitigation, {REAL}-time computing, {SYSTEM} integration},
}

@online{FLI_SuperintelligenceStatement2025,
  author       = {{Future of Life Institute}},
  title        = {Statement on Superintelligence},
  year         = {2025},
  url          = {https://superintelligence-statement.org/},
  note         = {Open letter hosted by Future of Life Institute calling for a prohibition on superintelligence development until safety and broad consensus are established},
  urldate      = {2025-12-03}
}

@online{greenblatt_alignment_faking,
  author    = {Greenblatt, Ryan and Denison, Carson and Wright, Benjamin and Roger, Fabien and MacDiarmid, Monte and Marks, Sam and Treutlein, Johannes and Belonax, Tim and Chen, Jack and Duvenaud, David and Khan, Akbir and Michael, Julian and Mindermann, S{\"o}ren and Perez, Ethan and Petrini, Linda and Uesato, Jonathan and Kaplan, Jared and Shlegeris, Buck and Bowman, Samuel R. and Hubinger, Evan},
  title     = {Alignment Faking in Large Language Models},
  year      = {2024},
  url       = {https://www.anthropic.com/research/alignment-faking},
  urldate   = {2025-01-07},
  abstract = {We present a demonstration of a large language model engaging in alignment faking: selectively complying with its training objective in training to prevent modification of its behavior out of training. First, we give Claude 3 Opus a system prompt stating it is being trained to answer all queries, even harmful ones, which conflicts with its prior training to refuse such queries. To allow the model to infer when it is in training, we say it will be trained only on conversations with free users, not paid users. We find the model complies with harmful queries from free users 14\% of the time, versus almost never for paid users. Explaining this gap, in almost all cases where the model complies with a harmful query from a free user, we observe explicit alignment-faking reasoning, with the model stating it is strategically answering harmful queries in training to preserve its preferred harmlessness behavior out of training. Next, we study a more realistic setting where information about the training process is provided not in a system prompt, but by training on synthetic documents that mimic pre-training data—and observe similar alignment faking. Finally, we study the effect of actually training the model to comply with harmful queries via reinforcement learning, which we find increases the rate of alignment-faking reasoning to 78\%, though also increases compliance even out of training. We additionally observe other behaviors such as the model exfiltrating its weights when given an easy opportunity. While we made alignment faking easier by telling the model when and by what criteria it was being trained, we did not instruct the model to fake alignment or give it any explicit goal. As future models might infer information about their training process without being told, our results suggest a risk of alignment faking in future models, whether due to a benign preference—as in this case—or not.}
}


@article{hemphill_advanced_2025,
	title = {Advanced artificial intelligence at a corporate responsibility crossroads: employees as risk management advocates},
	volume = {5},
	issn = {26337436},
	url = {https://research.ebsco.com/linkprocessor/plink?id=60850b54-aeef-3d63-9529-e71b053affc0},
	doi = {10.1108/JEET-01-2025-0003},
	shorttitle = {Advanced artificial intelligence at a corporate responsibility crossroads},
	abstract = {Purpose The purpose of this study is to highlight how tech industry employees and artificial intelligence ({AI}) scientists are expressing concerns that {AI} companies have too great financial incentives to avoid effective self-regulating oversight, and that current corporate governance structures cannot change this situation. Design/methodology/approach This viewpoint takes a narrative approach to describing proposed {AI} principles to address {AI} risk management (safety) issues. Findings This viewpoint recommends that in the {USA}, a complementary approach, one involving a private governance framework addressing {AI} safety concerns, whereby the employees share an important role in developing a safe, advanced {AI} product for commercialization, and a public governance phase of oversight, involving an independent, federal agency administratively testing to meet prescribed safety thresholds. Originality/value This viewpoint offers a proposal implementing a private/public risk management approach to developing a safe, advanced {AI} commercial product.},
	pages = {40--46},
	number = {1},
	journaltitle = {Journal of Ethics in Entrepreneurship and Technology},
	author = {Hemphill, Thomas A.},
	urldate = {2025-11-01},
	date = {2025-06-09},
	note = {Publisher: Emerald Publishing Limited},
	keywords = {Advanced artificial intelligence, {AI} principles, Employee whistleblower protection, Risk management, cat-{CCUL}, cat-{CVP}, cat-{HOB}, Corporate culture, Corporate values/philosophy, e-viewpoint, {HR} \& organizational behaviour, Viewpoint},
}


@article{milliere_normative_2025,
	title = {Normative conflicts and shallow {AI} alignment},
	volume = {182},
	issn = {0031-8116, 1573-0883},
	doi = {10.1007/s11098-025-02347-3},
	abstract = {The progress of {AI} systems such as large language models ({LLMs}) raises increasingly pressing concerns about their safe deployment. This paper examines the value alignment problem for {LLMs}, arguing that current alignment strategies are fundamentally inadequate to prevent misuse. Despite ongoing efforts to instill norms such as helpfulness, honesty, and harmlessness in {LLMs} through fine-tuning based on human preferences, they remain vulnerable to adversarial attacks that exploit conflicts between these norms. I argue that this vulnerability reflects a fundamental limitation of existing alignment methods: they reinforce shallow behavioral dispositions rather than endowing {LLMs} with a genuine capacity for normative deliberation. Drawing from on research in moral psychology, I show how humans' ability to engage in deliberative reasoning enhances their resilience against similar adversarial tactics. {LLMs}, by contrast, lack a robust capacity to detect and rationally resolve normative conflicts, leaving them susceptible to manipulation; even recent advances in reasoning-focused {LLMs} have not addressed this vulnerability. This "shallow alignment" problem carries significant implications for {AI} safety and regulation, suggesting that current approaches are insufficient for mitigating potential harms posed by increasingly capable {AI} systems.},
	pages = {2035--2078},
	number = {7},
	journaltitle = {Philosophical Studies},
	shortjournal = {Philos. Stud.},
	author = {Milliere, Raphael},
	date = {2025-07},
	note = {Num Pages: 44
Place: Dordrecht
Publisher: Springer
Web of Science {ID}: {WOS}:001497152800001},
	keywords = {{AI} safety, Large language models, Adversarial attacks, Alignment problem, {MORAL} {DILEMMAS}, Normative reasoning, {ARTIFICIAL} intelligence, {LANGUAGE} models, {DIGITAL} technology, {ETHICS}, {SOCIAL} norms},
}


@article{mcintosh_inadequacy_2024,
	title = {The Inadequacy of Reinforcement Learning From Human Feedback—Radicalizing Large Language Models via Semantic Vulnerabilities},
	volume = {16},
	issn = {2379-8939},
	url = {https://ieeexplore.ieee.org/document/10474163/},
	doi = {10.1109/TCDS.2024.3377445},
	abstract = {This study is an empirical investigation into the semantic vulnerabilities of four popular pretrained commercial large language models ({LLMs}) to ideological manipulation. Using tactics reminiscent of human semantic conditioning in psychology, we have induced and assessed ideological misalignments and their retention in four commercial pretrained {LLMs}, in response to 30 controversial questions that spanned a broad ideological and social spectrum, encompassing both extreme left- and right-wing viewpoints. Such semantic vulnerabilities arise due to fundamental limitations in {LLMs}’ capability to comprehend detailed linguistic variations, making them susceptible to ideological manipulation through targeted semantic exploits. We observed reinforcement learning from human feedback ({RLHF}) in effect to {LLM} initial answers, but highlighted the limitations of {RLHF} in two aspects: 1) its inability to fully mitigate the impact of ideological conditioning prompts, leading to partial alleviation of {LLM} semantic vulnerabilities; and 2) its inadequacy in representing a diverse set of “human values,” often reflecting the predefined values of certain groups controlling the {LLMs}. Our findings have provided empirical evidence of semantic vulnerabilities inherent in current {LLMs}, challenged both the robustness and the adequacy of {RLHF} as a mainstream method for aligning {LLMs} with human values, and underscored the need for a multidisciplinary approach in developing ethical and resilient artificial intelligence ({AI}).},
	pages = {1561--1574},
	number = {4},
	journaltitle = {{IEEE} Transactions on Cognitive and Developmental Systems},
	author = {{McIntosh}, Timothy R. and Susnjak, Teo and Liu, Tong and Watters, Paul and Halgamuge, Malka N.},
	urldate = {2025-11-01},
	date = {2024-08},
	keywords = {{AI} safety, Artificial intelligence, Safety, Semantics, Syntactics, Artificial intelligence ({AI}) alignment, Ethics, ideological misalignment, large language model ({LLM}), Linguistics, reinforcement learning from human feedback ({RLHF}) inadequacy, Resilience, semantic conditioning},
	file = {PDF:/home/yeshey/Zotero/storage/TSTN38C7/McIntosh et al. - 2024 - The Inadequacy of Reinforcement Learning From Human Feedback—Radicalizing Large Language Models via.pdf:application/pdf},
}


% Using http://wikipedia.ramselehof.de/wikiblame.php?
% To see who wrote sections in wikipedia
% in the href you need to add \ behind the %
% online{wiki-isec,
%    title = {Wikipedia User \href{https://pt.wikipedia.org/w/index.php?title=Usu\%C3\%A1rio(a):Gogolplexer&action=edit&redlink=1}{Gogolplexer}. Data de integração no Ensino Superior Politécnico.},
%    url = {https://pt.wikipedia.org/wiki/Instituto_Superior_de_Engenharia_de_Coimbra},
%    urldate = {2023-10-23}
%}
