&nbsp;

| Database | N   | Paper Title | Notes | Filtro 5 | AI Summary |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **ACM** | 1   | A Framework for Cryptographic Verifiability of End-to-End AI Pipelines | Using hashes and cryptography to certify each step of the AI pipeline | Y   | The paper proposes a framework for verifiable AI pipelines using cryptographic methods to ensure transparency, trust, and auditability throughout the AI lifecycle. |
| **ACM** | 2   | Gaps in the Safety Evaluation of Generative AI | "modality gap" - no safety for non-text  <br><br/>"safety coverage gap"  <br><br/>"context gap" | Y   | This paper reviews over 200 safety-related evaluations of generative AI systems and identifies three "evaluation gaps": a modality gap, a risk coverage gap, and a context gap. |
| **ACM** | 3   | Leveraging LLMs for Memory Forensics: A Comparative Analysis of Malware Detection | Using LLMs to assist with windows memory forensics, ex with "volatility3". | N   | This paper integrates Large Language Models into memory forensics workflows to detect malware, finding that reasoning-based models perform well but have high false-positive rates. |
| **ACM** | 4   | Mapping the individual, social and biospheric impacts of Foundation Models | Argues that the current focus has been a lot on Existential threat of AI, disregarding other frameworks like "social, political, and environmental dimensions of foundation models and generative AI"  <br><br/>"foundational models" - models that have been trained on a lot of data to act on many downstream tasks | Y   | This paper offers a critical framework to account for the social, political, and environmental dimensions of foundation models and generative AI, identifying 14 categories of risks and harms. |
| **ACM** | 5   | Policy Maps: Tools for Guiding the Unbounded Space of LLM Behaviors |     | Y (acho) | This paper introduces "Policy Maps," a tool for guiding and evaluating the behavior of Large Language Models to improve AI safety and policy adherence. |
| **ACM** | 6   | Public vs Private Bodies: Who Should Run Advanced AI Evaluations and Audits? A Three-Step Logic Based on Case Studies of High-Risk Industries | The title | Y   | Drawing from nine other high-risk industries, this paper recommends that public bodies should be directly involved in safety-critical AI model evaluations, while a private market can handle other audits under public oversight. |
| **ACM** | 7   | Responsible AI Engineering from a Data Perspective (Keynote) | Keynote | N   | This keynote introduces a responsible AI engineering approach to address system-level challenges, with a focus on tackling data-related issues within the context of responsible AI. |
| **ACM** | 8   | When Testing AI Tests Us: Safeguarding Mental Health on the Digital Frontlines | How to protect the well-being of people who red-team AI systems and test them for harmful output.  <br><br/>Has references - How LLMs can take people to suicide | N (i think) | This paper discusses the mental health challenges faced by individuals involved in testing and red-teaming generative AI systems and proposes safeguards for their well-being. |
| **EBSCO** | 9   | Advanced artificial intelligence at a corporate responsibility crossroads: employees as risk management advocates | Coorporate responsibility, it's basically saying what I am saying in my thesis | Y   | This viewpoint highlights the concerns of tech industry employees and AI scientists that AI companies lack effective self-regulating oversight and proposes a private/public risk management approach where employees play a key role. |
| **EBSCO** | 10  | AI safety of film capacitors | Como usar AI para film capacitors | N   | This paper discusses the potential AI safety hazards in the design, operation, and evaluation of film capacitors and proposes a human-AI common impact and multi-dimensional evaluation to mitigate these risks. |
| **EBSCO** | 11  | Are Goverments Prepared to Keep AI Safe? UK Prime Minister RISHI SUNAK in Conversation With ELON MUSK | só transcript de uma conversa entre 2 billionarios  <br>https://www.youtube.com/watch?v=R2meHtrO1n8 | N   | This article presents a conversation between UK Prime Minister Rishi Sunak and Elon Musk about the preparedness of governments to ensure AI safety. |
| **EBSCO** | 12  | BRIDGING THE GLOBAL DIVIDE IN AI REGULATION: A PROPOSAL FOR A CONTEXTUAL, COHERENT, AND COMMENSURABLE FRAMEWORK. | Talks about a LOT of laws around different countries and how they can interop | Y   | This paper argues for a context-specific approach to AI regulation and decentralized governance, proposing a "3C" framework (contextual, coherent, and commensurable) to enhance the systematicity and interoperability of international norms. |
| **EBSCO** | 13  | ChatGPT Without a Safety Net. | More about how to use it for learning, its small and easy to read | Y   | The article discusses the lack of a safety net for generative AI like ChatGPT, highlighting issues of accuracy, bias, and the failure of plagiarism policies to keep up with academic misconduct. |
| **EBSCO** | 14  | Defending Against AI Threats with a User-Centric Trustworthiness Assessment Framework | Making a framework for users to keep track of how trustworthy different AIs are | Y (or maybe no) | This study introduces a user-centric framework for assessing the trustworthiness of AI applications, evaluating dimensions such as transparency, security, privacy, and ethics. |
| **EBSCO** | 15  | Evaluating alignment in large language models: a review of methodologies | Red teaming, deceptive behaviour, aligment. Current methds to evaluate | Y   | This paper reviews four primary methodologies for evaluating alignment in Large Language Models (LLMs): human feedback, adversarial testing, AI red teaming, and the constitutional approach to AI safety. |
| **EBSCO** | 16  | Field-building and the epistemic culture of AI safety. | Addresses how there is a big community of people that are dispersed and concerned about AI as an exestencial risks and how they may have big influence | Y   | This paper analyzes the emerging field of "AI safety," arguing that an "AI safety epistemic community" has formed through mutually reinforcing community-building and knowledge production practices. |
| **EBSCO** | 17  | Harnessing the Power of AI. | It seems to just talk about AI regulation biden enacted with an executive order in the US? its small, so I can read ig, might be autdated bc of trump | Y (but you dont need to) | This article discusses President Biden's Executive Order #14110 on managing the risks of AI, which establishes new standards for AI safety and security. |
| **EBSCO** | 18  | HFES Added to AI Safety Institute Consortium. | Just a news article? The Human Factors and Ergonomics Society joined NIST’s AI Safety Institute Consortium to help develop standardized, interoperable methods for trustworthy and responsible AI. | N   | The article reports that the Human Factors and Ergonomics Society (HFES) has joined the AI Safety Institute Consortium to contribute to the development of measurement science for trustworthy AI. |
| **EBSCO** | 19  | Human biases and remedies in AI safety and alignment contexts | Talks about biases, what we should focuse on etc | Y   | This paper discusses how cognitive biases can affect the perception of AI risks and decision-making in AI development, safety, and governance, and reviews potential remedies. |
| **EBSCO** | 20  | Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations. | How LLMs can moniter their own neural cognition process. Finds that they can only control a subset, and gives a way to measure this | Y   | This paper introduces a neurofeedback paradigm to quantify the ability of LLMs to report and control their internal activation patterns, with implications for AI safety and oversight. |
| **EBSCO** | 21  | Layered Alignment. | Calls the aligment problem the principeled agent problem. Defines AI aligment problem how I defined it. Talks about complexity theory and uses it to provide a layered approach to tackle the complex ai aligment problem | Y   | Drawing on complexity theory and the principal-agent problem, this article argues for a "layered" approach to AI alignment, using a variety of mechanisms to address different aspects of the alignment problem. |
| **EBSCO** | 22  | Local US officials' views on the impacts and governance of AI: Evidence from 2022 and 2023 survey waves | Survey, on the USA, acho que posso ver por alto. | Y (por alto) | This paper presents survey findings on the views of local US policymakers regarding the impact and regulation of AI, showing a mix of concern and optimism, and a feeling of being underprepared for AI-related decisions. |
| **EBSCO** | 23  | NOVEL CORPORATE GOVERNANCE STRUCTURES. | Fala de como Companhias de AI nao se conseguem self-regulate como ja sabiamos | Y (lê com AI) | This article explores the innovative corporate governance models of leading AI startups like OpenAI, Anthropic, and xAI, and proposes a board-level AI Safety Committee for AI startups. |
| **EBSCO** | 24  | Philosophical Investigations into AI Alignment: A Wittgensteinian Framework. | DEVIAS ABRIR A TUA TESE DA MESMA FORMA QUE ESTE ABRIU. Fala como podemos alinhar o gajo | Y   | This paper argues that Wittgenstein's philosophy of language and rule-following can inform and improve the AI alignment problem by identifying categories that should be controlled in training data and hard-coded guardrails. |
| **EBSCO** | 25  | Research and analysis on safety design of AI technology applied to space system | No access to the paper | N   | This paper analyzes the key aspects of AI safety design for space systems, considering the influence of the cosmic environment on data, models, and the overall system. |
| **EBSCO** | 26  | Responsible AI in biotechnology: balancing discovery, innovation and biosecurity risks. | Not what Im looking for | N   | This review examines the dual-use potential of AI in bioengineering and the necessary safeguards to prevent misuse for bioweapons while fostering innovation. |
| **EBSCO** | 27  | Risks of Using Artificial Intelligence in Creating the Image of Politicians and in Electoral Campaigns. | Risks of the title, disinformation, manipulating voters. | N   | This paper examines the risks of using AI, such as deepfakes and microtargeting, in politics and electoral campaigns, and discusses potential ways to minimize these threats. |
| **EBSCO** | 28  | The harms of terminology: why we should reject so-called “frontier AI” | A guy mad because people are using "frontier AI" to talk about exestencial risk in AI | Y   | This paper argues that the term "frontier AI" is harmful AI hype that shifts focus from actual, present-day harms of generative AI to hypothetical doomsday scenarios, while also invoking a colonial mindset. |
| **EBSCO** | 29  | The PacifAIst Benchmark: Do AIs Prioritize Human Survival over Their Own Objectives? | Evaluetes biggest AI models on if they're aligned or not | Y   | This paper introduces a benchmark to test whether AI systems prioritize human safety over their own objectives in scenarios involving self-preservation, resource acquisition, and deception. |
| **EBSCO** | 30  | The paradox of AI accelerationism and the promise of public interest AI. | short piece on ai accelerationism | Y   | This article contrasts AI accelerationism, which advocates for unrestrained AI development, with public interest AI, which promotes equitable, sustainable, and democratically accountable technological progress. |
| **EBSCO** | 31  | This Is Not a Game: The Addictive Allure of Digital Companions. | Protect the children | Y   | This article examines the risks of children forming deep emotional bonds with AI companions, which can distort social development and foster unhealthy dependencies, and calls for regulatory intervention. |
| **EBSCO** | 32  | Why do experts disagree on existential risk? A survey of AI experts | title | Y   | This paper presents a survey of AI experts on their views of existential risk from AI, finding that they cluster into two groups—"AI as controllable tool" and "AI as uncontrollable agent"—with varying levels of familiarity with AI safety concepts. |
| **IEEE** | 33  | A Survey on Verification and Validation, Testing and Evaluations of Neurosymbolic Artificial Intelligence | Symbolic and sub symbolic AI | Y   | This survey explores how neurosymbolic AI, which combines symbolic and subsymbolic approaches, can ease the verification and validation process for AI systems, making them safer and more trustworthy. |
| **IEEE** | 34  | Agent-SiMT: Agent-Assisted Simultaneous Translation With Large Language Models | About using LLMs for translations | N   | This paper introduces Agent-SiMT, a framework for simultaneous machine translation that combines a traditional SiMT model for policy decisions with a large language model for translation generation. |
| **IEEE** | 35  | An Effective Iterative Statistical Fault Injection Methodology for Deep Neural Networks | Introduces bit faults to see how they behave, para ver em situações de alta radiação se podem ser usados por exemplo | N   | This research presents an iterative statistical fault injection approach to efficiently estimate the failure rates of convolutional neural networks caused by random hardware faults. |
| **IEEE** | 36  | Anomalous State Sequence Modeling to Enhance Safety in Reinforcement Learning | How to train in order for the agent to be more safe, for example in a self driving car scenario | Y   | This paper proposes a safe reinforcement learning approach that uses an anomaly detection model trained on safe state sequences to identify and penalize potentially unsafe states in a new environment. |
| **IEEE** | 37  | CVaR-Constrained Policy Optimization for Safe Reinforcement Learning | Same as above | Y(but its big) | This paper proposes a safe reinforcement learning algorithm that considers the Conditional Value-at-Risk (CVaR) of cumulative costs as a safety constraint to ensure that agents avoid high-cost outcomes. |
| **IEEE** | 38  | Detection of Unknown-Unknowns in Human-in-Loop Human-in-Plant Safety Critical Systems | Way to detect if the inputs of the AI are correct according to physical reality | N   | This article introduces a methodology for preemptively identifying "unknown-unknown" errors in AI-enabled autonomous systems by using physics-guided models and conformal inference to detect violations of physical laws. |
| **IEEE** | 39  | Establishing Rigorous Certification Standards: A Systematic Methodology for AI Safety-Critical Systems in Military Aviation | Too specific | N   | This article presents a systematic methodology for developing a certification standard for AI safety-critical systems in military aviation by amalgamating and analyzing military and civil airworthiness references. |
| **IEEE** | 40  | Explainable AI for Safe and Trustworthy Autonomous Driving: A Systematic Review | Explainable AI for autonomus driving. Include but **don't need to read**  <br>Based on their review, the authors propose a new framework called **SafeX**, which integrates these different XAI techniques into the AD system to provide both robust safety monitoring and clear explanations to users. | Y   | This paper provides a systematic literature review of explainable AI (XAI) methods for safe and trustworthy autonomous driving, proposing a conceptual framework called SafeX to integrate these methods. |
| **IEEE** | 41  | Improving Co-Decoding Based Security Hardening of Code LLMs Leveraging Knowledge Distillation | Make the models generate safer code with less vulnerabilities  <br><br/>Also include but **also dont need to read** | Y   | This paper proposes CoSec+, a framework to improve the security of code-generating Large Language Models by using a small, independent security model to guide the decoding process, enhancing both security and functional correctness. |
| **IEEE** | 42  | Instance-Consistent Fair Face Recognition | Make image recognition fair. include but u can also breefly just see it | Y   | This work proposes a method for fair face recognition that aims to achieve complete instance fairness on both false positive and true positive rates by introducing customized instance margins during training. |
| **IEEE** | 43  | The Inadequacy of Reinforcement Learning From Human Feedback—Radicalizing Large Language Models via Semantic Vulnerabilities | Usam carefully crafted prompts to make the model be extrem left or right wing.. Also found that by default it is left wing (whatever that means) | Y   | This study investigates the semantic vulnerabilities of four large language models, showing that they can be ideologically manipulated and that Reinforcement Learning from Human Feedback (RLHF) is insufficient to fully mitigate this. |
| **Scopus** | 44  | Artificial Intelligence: Approaches to Safety | align human values | Y   | This paper reviews research in AI safety, focusing on areas where philosophers can contribute, including ethical AI, scalable oversight, interpretability, and corrigibility. |
| **Scopus** | 45  | Bias Amplification to Facilitate the Systematic Evaluation of Bias Mitigation Methods | Bias between groups | Y   | This work presents two approaches to systematically amplify bias in AI models to allow for the structured evaluation and comparison of different bias mitigation methods. |
| **Scopus** | 46  | Counter-productivity and suspicion: two arguments against talking about the AGI control problem | Absolutly bonkers arguments about how we should not talk about the AGI problem because talking about it will make the agent more able to fight against us  <br><br/>Do you even need to read this? | Y   | This paper presents two arguments against open discussion of the AGI control problem: that it could be counter-productive by informing a misaligned AGI, and that it could make humanity appear threatening to an AGI. |
| **Scopus** | 47  | Deontology and safe artificial intelligence | Interesting | Y   | This paper argues that aligning advanced AI with deontological morality does not guarantee safety, and that safety should be prioritized when it conflicts with moral alignment. |
| **Scopus** | 48  | Dual-use capabilities of concern of biological AI models. | Argues about AI danger as a tool. Because empowering humans may mean humans can use it for good or bad. | y   | This paper argues that experience from dual-use research in life sciences can inform the evaluation of AI models trained on biological data to mitigate the risk of misuse for developing biological threats. |
| **Scopus** | 49  | Ethical and responsible use of GenAI in research context | Talks about biases | I want to say no Y | This article presents a governance model for the responsible and integrated implementation of Generative AI in research environments. |
| **Scopus** | 50  | Evaluating Interpretable Methods via Geometric Alignment of Functional Distortions | If you have an interpretable model, how do you make sure it is actually telling you what is happening | Y   | The authors propose a new evaluation criterion for the faithfulness of explanations of AI models, centered on explanation-to-model alignment and incorporating principles from differential geometry. |
| **Scopus** | 51  | Governance fix? Power and politics in controversies about governing generative AI | governance | Y   | The article examines early international governance initiatives for generative AI, finding them to be dominated by a few developed countries and framed in terms of risk management, neglecting public participation. |
| **Scopus** | 52  | Harnessing Metacognition for Safe and Responsible AI | Method to make the AI system follow its own morality system better (if it is misaligned it sounds like it will be able to more surely be missaligned) | Y (but just skim it) | The paper explores integrating metacognition ("thinking about thinking") into AI systems to enhance their safety, ethics, and responsibility by improving self-assessment and adaptability. |
| **Scopus** | 53  | Local AI Governance: Addressing Model Safety and Policy Challenges Posed by Decentralized AI | Regulating local AI? Current approaches to AI safety fail when everyone can run an AI at home (i dont like the way this sounds) | Y   | The paper addresses the challenges to AI safety and regulation posed by the rise of open-source AI models running on personal devices and proposes technical and policy solutions for a decentralized AI ecosystem. |
| **Scopus** | 54  | Misalignment or misuse? The AGI alignment tradeoff | If we align it with us, then we might misuse it. So how do We prevent that | Y   | This paper defends the view that misaligned AGI poses catastrophic risks, while also arguing that aligned AGI creates a substantial risk of misuse by humans, and explores this tradeoff for different alignment techniques. |
| **Scopus** | 55  | On the Controllability of Artificial Intelligence: An Analysis of Limitations | Can we even control it????? | YYY | This paper argues that advanced AI cannot be fully controlled and discusses the consequences for humanity and AI safety research. |
| **Scopus** | 56  | Psychopathia Machinalis: A Nosological Framework for Understanding Pathologies in Advanced Artificial Intelligence | Mental desieses that exist in LLMs | Y   | This paper introduces a framework for classifying and understanding maladaptive behaviors in advanced AI, drawing analogies to human psychopathologies to improve AI safety and interpretability. |
| **Scopus** | 57  | The cognitive mirror: a framework for AI-powered metacognition and self-regulated learning | a model that risks hindering  <br>genuine learning by fostering cognitive offloading  <br><br/>You can take a look, but basically argues that it would be better to have machines learn through what the student teaches it, instead of having machines teach the student. At least for the student to learn | Y   | This study proposes a shift from viewing AI as an "oracle" to a "Cognitive Mirror" that acts as a teachable novice, reflecting the quality of a learner's explanation to foster deeper learning. |
| **Scopus** | 58  | The Global AI Dilemma: Balancing Innovation and Safety in the European Union, the United States, and China | China, EU and EUA approaches to AI | Y   | This paper examines the differing regulatory approaches to AI in the EU (risk prevention), the US (innovation-focused), and China (AI leadership with political oversight), and calls for international cooperation. |
| **Scopus** | 59  | When code isn't law: rethinking regulation for artificial intelligence | How we should regulate it | Y   | This article proposes an adapted regulatory model for AI systems, which are difficult to analyze and audit, suggesting consolidated authority, licensing, and formal verification. |
| **Web of Science** | 60  | A New Perspective on AI Safety Through Control Theory Methodologies | More safety frameworks | Y   | This article proposes a new perspective on AI safety by leveraging control theory methodologies to analyze and ensure the safety of AI systems, particularly in safety-critical cyber-physical systems. |
| **Web of Science** | 61  | A Systematic Literature Review on AI Safety: Identifying Trends, Challenges, and Future Directions | the main concerns include explainability, interpretability, robustness, reliability, fairness, bias,  <br>and adversarial attacks | Y   | This survey provides a systematic overview of AI safety research, highlighting the importance of designing AI systems with a focus on safety throughout their lifecycle and identifying key concerns such as explainability, robustness, and fairness. |
| **Web of Science** | 62  | A Systematic Review of Open Datasets Used in Text-to-Image (T2I) Gen AI Model Safety | bias | Y (but just skim it) | This paper presents a comprehensive review of key datasets used in text-to-image generative AI safety research, detailing their composition, diversity, and the quality of harm labeling to identify gaps for future research. |
| **Web of Science** | 63  | AI Regulation Has Its Own Alignment Problem: The Technical and Institutional Feasibility of Disclosure, Registration, Licensing, and Auditing | Regulation in EUA | Y   | This article assesses four dominant proposals for AI regulation in the United States, arguing that they often suffer from misalignment with societal values and face significant technical and institutional feasibility challenges. |
| **Web of Science** | 64  | AI safety for everyone | Apanhado de outros trabalhos | Y   | This paper argues for an inclusive and pluralistic conception of AI safety that goes beyond a focus on existential risk and addresses the wide spectrum of immediate and practical safety challenges with current AI systems. |
| **Web of Science** | 65  | Artificial intelligence in dentistry: insights and expectations from Swiss dental professionals | AI for dentists | N   | This study surveyed Swiss dentists about their opinions on AI in dentistry, revealing that while a significant portion uses AI, there are concerns about its utility and potential for job replacement, especially among older dentists. |
| **Web of Science** | 66  | Bounding Reason: Inferentialism, Naturalism, and the Discursive Agency of LLMs | Challenges Orthogonality thesis | Y   | This paper challenges the idea that AI intelligence and goals are independent, arguing that the linguistic competence of LLMs suggests their rationality may be constrained by human discursive practices. |
| **Web of Science** | 67  | Can there be responsible AI without AI liability? Incentivizing generative AI safety through ex-post tort liability under the EU AI liability directive | Unpredictable LLMs and liability | Y   | This article argues for the necessity of a responsive tort liability system, like the proposed EU AI Liability Directive, to ensure AI safety and responsible AI, especially as AI systems become more autonomous. |
| **Web of Science** | 68  | Civilizing and Humanizing Artificial Intelligence in the Age of Large Language Models | Make AIs act more like humans??? | Y   | This special issue introduction outlines two research directions for AI safety: "civilizing" AI to mitigate adverse behaviors and "humanizing" AI to align with human ethics and societal norms. |
| **Web of Science** | 69  | Deceptively simple: An outsider's perspective on natural language processing | They found shoking antisemitism? | Y   | This article presents a collection of deceptively simple ideas that address challenges in computational social science and generative AI safety, including a framework for political polarization and a bias audit framework for large language models. |
| **Web of Science** | 70  | Existential Risk from Transformative Ai: An Economic Perspective | What can we do, in an economic context, to deal with an exestencial threat like AI? Also pascal mugging? | Y   | This paper argues that transformative AI poses an imminent existential risk and provides economic perspectives on addressing this risk, including cost-effectiveness, fairness, and policy actions. |
| **Web of Science** | 71  | Governance efficiency and upgrade pathways of international generative AI policies and regulations | no access | N   | This paper analyzes the governance efficiency of generative AI policies and regulations from 24 countries, identifying key factors and pathways to enhance their effectiveness. |
| **Web of Science** | 72  | Governance of Generative AI | Touches on everything, but basically how to regulate generative AI | Y   | This introductory article to a special issue on the governance of generative AI outlines the key risks and governance challenges of this technology and proposes a comprehensive framework for its governance. |
| **Web of Science** | 73  | Helpful, harmless, honest? Sociotechnical limits of AI alignment and safety through Reinforcement Learning from Human Feedback | Can you align AI with human feedback reeiforcement learning? | Y   | This paper critically evaluates the use of Reinforcement Learning from Human Feedback (RLHF) to align AI with human values, highlighting its limitations in capturing the complexity of human ethics. |
| **Web of Science** | 74  | Normative conflicts and shallow AI alignment | They're too dumb to know that they can be manipulated, while we humans can do deliberate reasoning they cant. says current aliment techniques are shallow | Y   | The paper argues that current AI alignment strategies create "shallow alignment" that is vulnerable to adversarial attacks exploiting conflicts between instilled norms, as they lack a genuine capacity for normative deliberation. |
| **Web of Science** | 75  | Personality testing of large language models: limited temporal stability, but highlighted prosociality | Study the personality profile of LLMs | Y (?) | This study investigated the personality of seven large language models, finding varying levels of temporal stability in their responses and a tendency to display a socially desirable, prosocial profile. |
| **Web of Science** | 76  | Security practices in AI development | Improvements to current ai development security practicies | Y   | This paper critically investigates how security practices in AI, such as alignment and red teaming, attempt to bridge the gap between the capabilities of security tools and desired safety guarantees, identifying shortcomings in diversity and participation. |
| **Web of Science** | 77  | Survey evidence on public support for AI safety oversight. | talks about all the safety issues  I don't really care about:  <br><br/>A number of AI safety concerns are being increasingly discussed by experts, including misinformation,  <br>invasion of privacy, job displacement, and criminal misuse  <br><br/>   <br><br/>Mas diz que o pessoal quer mais ontrolo de AI claro | Y   | This paper presents survey data from Germany and Spain showing broad public support for strict oversight of commercial AI research, with support being associated with factors like age and anticipated job displacement. |
| **Web of Science** | 78  | Terra Incognita: The Governance of Artificial Intelligence in Global Perspective | Mais jargon | Y   | This paper argues that AI safety is AI governance and calls for a dual-track interactive strategy to contain the potential negative consequences of AI, emphasizing the need to bridge the gap between theory and practice and between STEM and the humanities. |
| **Web of Science** | 79  | Transformative Impact of the EU AI Act on Maritime Autonomous Surface Ships | too specific | N   | This study examines the impact of the EU AI Act on the development of Maritime Autonomous Surface Ships (MASS), emphasizing the need to balance AI safety and ethical responsibility with innovation in the maritime industry. |

- Criterios de exclusão Keynotes:
- Posso colocar um link na minha tabela do corpo da tese para a tabela completa com todos os criterios de exclusao e inclusao

![8b9b9ba035658c8eb4d7a16256cf44f6.png](:/cda881d5f63f445287f8a3b3f809c67e)

Posso seguir isto mas faria o snowballing depois do asbtract screened

&nbsp;

Fazer ao Mesmo tempo:

![fc5ba28c8ca82772b4cf0ca5ad627479.png](:/40ccc7655bf34b88b37837424a9ce99f)

Adicionar aqui outros critérios de exclusão como ser em inglês ou não ter o artigo disponivel etc.

Colocar ao menos uma lista ao pé da tua tabela para quais é que são is inclusion e exclusion criteria sobre toda a research nao sobre os filtros especificos.

AGI Control Problem

Use this phrase:

- a model that risks hindering
genuine learning by fostering cognitive offloading

&nbsp;

All the safety I dont care about:

- A number of AI safety concerns are being increasingly discussed by experts, including misinformation,
invasion of privacy, job displacement, and criminal misuse

| Title | Nº  | Alignment | Governance, regulation & Policy | Auditing Safety & oversight | Explainability / Interpretability | Bias & Fairness | Liability & Ethics | Human & Societal Impacts | No access |
| --- | --- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| A Framework for Cryptographic Verifiability of End-to-End AI Pipelines | 1   |     | X   | X   |     |     |     |     |     |
| Gaps in the Safety Evaluation of Generative AI | 2   |     | X   | X   |     |     |     |     |     |
| Leveraging LLMs for Memory Forensics: A Comparative Analysis of Malware Detection | 3   |     |     |     |     |     |     |     |     |
| Mapping the individual, social and biospheric impacts of Foundation Models | 4   |     | X   |     |     |     |     | X   |     |
| Policy Maps: Tools for Guiding the Unbounded Space of LLM Behaviors | 5   |     | X   |     |     |     |     |     |     |
| Public vs Private Bodies: Who Should Run Advanced AI Evaluations and Audits? A Three-Step Logic Based on Case Studies of High-Risk Industries | 6   |     | X   | X   |     |     |     | X   |     |
| Responsible AI Engineering from a Data Perspective (Keynote) | 7   |     |     |     |     |     |     |     |     |
| When Testing AI Tests Us: Safeguarding Mental Health on the Digital Frontlines | 8   |     |     |     |     |     |     | X   |     |
| Advanced artificial intelligence at a corporate responsibility crossroads: employees as risk management advocates | 9   |     | X   |     |     |     |     | X   |     |
| AI safety of film capacitors | 10  |     | X   |     |     |     |     |     |     |
| Are Goverments Prepared to Keep AI Safe? UK Prime Minister RISHI SUNAK in Conversation With ELON MUSK | 11  |     | X   |     |     |     |     |     |     |
| BRIDGING THE GLOBAL DIVIDE IN AI REGULATION: A PROPOSAL FOR A CONTEXTUAL, COHERENT, AND COMMENSURABLE FRAMEWORK. | 12  |     | X   |     |     |     |     | X   |     |
| ChatGPT Without a Safety Net. | 13  |     |     |     |     |     |     | X   |     |
| Defending Against AI Threats with a User-Centric Trustworthiness Assessment Framework | 14  |     |     |     |     | X   |     | X   |     |
| Evaluating alignment in large language models: a review of methodologies | 15  | X   |     |     | X   |     | X   |     |     |
| Field-building and the epistemic culture of AI safety. | 16  |     |     |     |     |     |     | X   |     |
| Harnessing the Power of AI. | 17  |     | X   |     |     |     |     |     |     |
| HFES Added to AI Safety Institute Consortium. | 18  |     |     |     |     |     |     |     |     |
| Human biases and remedies in AI safety and alignment contexts | 19  | X   |     |     |     | X   |     | X   |     |
| Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations. | 20  | X   |     |     | X   |     |     |     |     |
| Layered Alignment. | 21  | X   |     |     |     |     |     |     |     |
| Local US officials' views on the impacts and governance of AI: Evidence from 2022 and 2023 survey waves | 22  |     |     |     |     |     |     | X   |     |
| NOVEL CORPORATE GOVERNANCE STRUCTURES. | 23  |     | X   |     |     |     |     |     |     |
| Philosophical Investigations into AI Alignment: A Wittgensteinian Framework. | 24  | X   |     |     |     | X   |     |     |     |
| Research and analysis on safety design of AI technology applied to space system | 25  |     |     |     |     |     |     |     | X   |
| Responsible AI in biotechnology: balancing discovery, innovation and biosecurity risks. | 26  |     |     |     |     |     |     |     |     |
| Risks of Using Artificial Intelligence in Creating the Image of Politicians and in Electoral Campaigns. | 27  |     |     |     |     |     |     | X   |     |
| The harms of terminology: why we should reject so-called “frontier AI” | 28  |     | X   |     |     |     |     | X   |     |
| The PacifAIst Benchmark: Do AIs Prioritize Human Survival over Their Own Objectives? | 29  | X   |     |     |     |     |     |     |     |
| The paradox of AI accelerationism and the promise of public interest AI. | 30  |     | X   |     |     |     | X   | X   |     |
| This Is Not a Game: The Addictive Allure of Digital Companions. | 31  |     |     |     |     |     |     | X   |     |
| Why do experts disagree on existential risk? A survey of AI experts | 32  |     |     |     |     |     |     | X   |     |
| A Survey on Verification and Validation, Testing and Evaluations of Neurosymbolic Artificial Intelligence | 33  |     |     | X   | X   |     |     |     |     |
| Agent-SiMT: Agent-Assisted Simultaneous Translation With Large Language Models | 34  |     |     |     |     |     |     |     |     |
| An Effective Iterative Statistical Fault Injection Methodology for Deep Neural Networks | 35  |     |     | X   |     |     |     |     |     |
| Anomalous State Sequence Modeling to Enhance Safety in Reinforcement Learning | 36  |     |     | X   |     |     |     |     |     |
| CVaR-Constrained Policy Optimization for Safe Reinforcement Learning | 37  |     |     | X   |     |     |     |     |     |
| Detection of Unknown-Unknowns in Human-in-Loop Human-in-Plant Safety Critical Systems | 38  |     |     |     |     |     |     |     |     |
| Establishing Rigorous Certification Standards: A Systematic Methodology for AI Safety-Critical Systems in Military Aviation | 39  |     |     | X   |     |     |     |     |     |
| Explainable AI for Safe and Trustworthy Autonomous Driving: A Systematic Review | 40  |     |     |     | X   |     | X   | X   |     |
| Improving Co-Decoding Based Security Hardening of Code LLMs Leveraging Knowledge Distillation | 41  |     |     | X   |     |     |     |     |     |
| Instance-Consistent Fair Face Recognition | 42  |     |     |     |     | X   |     |     |     |
| The Inadequacy of Reinforcement Learning From Human Feedback—Radicalizing Large Language Models via Semantic Vulnerabilities | 43  | X   |     |     |     |     |     | X   |     |
| Artificial Intelligence: Approaches to Safety | 44  | X   |     |     |     |     |     |     |     |
| Bias Amplification to Facilitate the Systematic Evaluation of Bias Mitigation Methods | 45  |     |     |     |     | X   |     |     |     |
| Counter-productivity and suspicion: two arguments against talking about the AGI control problem | 46  | X   |     |     |     |     |     | X   |     |
| Deontology and safe artificial intelligence | 47  | X   |     |     |     |     | X   | X   |     |
| Dual-use capabilities of concern of biological AI models. | 48  |     |     | X   |     |     |     | X   |     |
| Ethical and responsible use of GenAI in research context | 49  |     |     |     |     | X   |     | X   |     |
| Evaluating Interpretable Methods via Geometric Alignment of Functional Distortions | 50  | X   |     |     | X   |     |     |     |     |
| Governance fix? Power and politics in controversies about governing generative AI | 51  |     | X   |     |     |     |     |     |     |
| Harnessing Metacognition for Safe and Responsible AI | 52  | X   |     |     | X   |     |     |     |     |
| Local AI Governance: Addressing Model Safety and Policy Challenges Posed by Decentralized AI | 53  |     | X   |     |     |     |     | X   |     |
| Misalignment or misuse? The AGI alignment tradeoff | 54  | X   |     |     |     |     |     |     |     |
| On the Controllability of Artificial Intelligence: An Analysis of Limitations | 55  | X   |     |     |     |     |     |     |     |
| Psychopathia Machinalis: A Nosological Framework for Understanding Pathologies in Advanced Artificial Intelligence | 56  | X   |     |     |     |     |     | X   |     |
| The cognitive mirror: a framework for AI-powered metacognition and self-regulated learning | 57  |     |     |     |     |     |     | X   |     |
| The Global AI Dilemma: Balancing Innovation and Safety in the European Union, the United States, and China | 58  |     | X   |     |     |     |     |     |     |
| When code isn't law: rethinking regulation for artificial intelligence | 59  |     | X   |     |     |     |     |     |     |
| A New Perspective on AI Safety Through Control Theory Methodologies | 60  |     | X   | X   |     |     |     |     |     |
| A Systematic Literature Review on AI Safety: Identifying Trends, Challenges, and Future Directions | 61  |     | X   | X   |     |     |     |     |     |
| A Systematic Review of Open Datasets Used in Text-to-Image (T2I) Gen AI Model Safety | 62  |     |     |     |     | X   |     |     |     |
| AI Regulation Has Its Own Alignment Problem: The Technical and Institutional Feasibility of Disclosure, Registration, Licensing, and Auditing | 63  |     | X   |     |     |     |     |     |     |
| AI safety for everyone | 64  |     | X   |     |     |     |     |     |     |
| Artificial intelligence in dentistry: insights and expectations from Swiss dental professionals | 65  |     |     |     |     |     |     | X   |     |
| Bounding Reason: Inferentialism, Naturalism, and the Discursive Agency of LLMs | 66  | X   |     |     |     |     |     | X   |     |
| Can there be responsible AI without AI liability? Incentivizing generative AI safety through ex-post tort liability under the EU AI liability directive | 67  |     | X   |     |     |     | X   |     |     |
| Civilizing and Humanizing Artificial Intelligence in the Age of Large Language Models | 68  |     |     |     |     |     |     | X   |     |
| Deceptively simple: An outsider's perspective on natural language processing | 69  |     |     |     |     | X   |     |     |     |
| Existential Risk from Transformative Ai: An Economic Perspective | 70  |     | X   |     |     |     |     | X   |     |
| Governance efficiency and upgrade pathways of international generative AI policies and regulations | 71  |     |     |     |     |     |     |     | X   |
| Governance of Generative AI | 72  |     | X   | X   |     |     |     | X   |     |
| Helpful, harmless, honest? Sociotechnical limits of AI alignment and safety through Reinforcement Learning from Human Feedback | 73  | X   |     |     |     |     |     |     |     |
| Normative conflicts and shallow AI alignment | 74  | X   |     | X   |     |     |     |     |     |
| Personality testing of large language models: limited temporal stability, but highlighted prosociality | 75  |     |     |     |     |     |     | X   |     |
| Security practices in AI development | 76  |     |     | X   |     |     |     |     |     |
| Survey evidence on public support for AI safety oversight. | 77  |     | X   |     |     |     |     | X   |     |
| Terra Incognita: The Governance of Artificial Intelligence in Global Perspective | 78  |     | X   |     |     |     |     |     |     |
| Transformative Impact of the EU AI Act on Maritime Autonomous Surface Ships | 79  |     | X   |     |     |     |     |     |     |