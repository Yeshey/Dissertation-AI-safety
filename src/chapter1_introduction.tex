\chapter{Introduction}\label{chap:introduction}
% \noindent You must use \textbackslash{}noindent at the beginning of the first paragraph in sections and subsections.

This chapter is meant to contextualize the work explored and tackled throughout the dissertation by precisely defining the problem statement, the intended objectives, and the initial motivation for picking this area.

\section{Motivation}

Up until now, every technological achievement merely represented an extension of our capabilities, fire allowed us to better realize our desires for food, shelter, the invention of the wheel allowed us to better realize our desires for traveling and moving materials, but never was there the possibility of there being an invention that could substitute the inventive process, an invention that invents.

AI might be the best thing that happens to us, or the worse thing, I want to make sure the AI systems we create are safe and human, and prevent them from dismantling being a net negative or even a societal ending project.

%\noindent Use Section tag for major section.
%    \subsection{This is a subsection}

%\noindent This is a citation: \cite{iscac}
%    \subsection{This is another subsection}
%\noindent This is a dummy text under a subsection.\par
%    This is the second paragraph.
\section{Problem}

Nowadays, although AI and machine learning systems are a lot more capable than humans in certain narrow domains, such as: chess, go, protein folding, etc. No AI is yet capable of general thinking like humans are, and the current societal structure mandates, through competition and rivalry, that there will be unfathomable amounts of money invested in trying to reach such capable systems. In this environment, safety is often disregarded, leading to openAI dismantling its AI safety team, safety standards very lax in xAI, allowing even for anti-semitic versions of Grok to come online for several hours. Reports like \cite{AISafetyReport2025} reveal how lacking our investment in AI safety is, and \cite{FLIAISafetyIndex2025} gives a failing grade to every major AI company today.

\section{Objectives}

In the world of AI safety, there's a lot of research surrounding AI alignment, preventing misbehavior in a distributional shift, explainable AI, etc. The proposed hypothesis is the following:
\begin{itemize}
	\item \textbf{Hypothesis}: If the current trajectory of AI development is deemed to be unsafe, then, in the present environment, where every big AI company is incentivized almost existentially into investing large swaths of money into this technology, no amount of AI safety research is enough, meaning that to truly guarantee safe and aligned AI systems for humans in the future, changes to the incentive structure that pushes companies in this direction are also required.
\end{itemize}
Therefore, this research will be conducted in a two-step SLR:
\begin{itemize}
	\item \textbf{SLR 1}: Aims at finding out if the current pursuit of AGI has natural dangers, how dangerous, and weather they are relevant in general or only narrow domains.
	\item \textbf{SLR 2}: If danger is found in the current approach, what are the incentives that keep steering and brought about the current situation? What are they sensitive to, and how can they be analyzed and changed?
\end{itemize}

Thus, this first literature review shall focus on work that has been carried out in analyzing the intent and inner objectives of current generalist models, for that purpose, the following research questions will be used to carry out the research on \textbf{SLR 1}:

\begin{enumerate}
	\item RQ1. What is the main focus of current AI safety research?
	\item RQ2. Witch are the current methods used to look into the alignment of current generalist models?
	\item RQ3. What conclusions regarding the safety of current models and safety of development and deployment processes can we assert from current research?
	\item RQ4. What metrics can be used to more clearly show potential problems with these systems?
\end{enumerate}

\section{Dissertation Structure}

\newpage