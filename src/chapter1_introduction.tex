\chapter{Introduction}\label{chap:introduction}
% \noindent You must use \textbackslash{}noindent at the beginning of the first paragraph in sections and subsections.

%

This chapter is meant to contextualize the work explored and tackled throughout the dissertation by precisely defining the problem statement, the intended objectives, and the initial motivation for picking this area.

\section{Motivation}

Up until now, every technological achievement merely represented an extension of our capabilities. Now, with AI, for the first time there is the prospect of a technological creation that would have its own goals and be capable of substituting the inventive process itself, rendering human capabilities obsolete.

The motivation of this work stems from the desire to keep the future human and human aligned.

\section{Problem}

The dangers of unaligned AI frequently start with Bostrom’s “Paperclip Maximizer” which demonstrates how an AI system with seemingly benign goals could still potentially lead to catastrophic outcomes if not properly aligned with human values. This thought experiment is rooted in the orthogonality thesis witch states that there is no inherent link between a system's goals and its ability to reach them.
Building on these concerns, Yampolskiy argues that it's definitionally impossible for humans to understand or control a super-intelligent agent\cite{yampolskiy_controllability_2022}. Such arguments have motivated calls for caution in the development of advanced AI systems, including public statements advocating for the suspension or regulation of certain forms of AI research until adequate safety guarantees can be established\cite{FLI_SuperintelligenceStatement2025}.

Today we have Multi-modal and Generalist LLMs capable of doing tasks in a large range of domains that exhibit early signs of instrumental convergence behavior\cite{greenblatt_alignment_faking}.

Furthermore, current societal structure mandates, through competition and rivalry, that there will be large amounts of money invested in trying to reach such capable systems, and there are rising concerns that AI companies have too great financial incentives to avoid effective self-regulating oversight\cite{hemphill_advanced_2025}. Witch is corroborated by several AI safety reports that reveal how lacking the approach of current AI companies is towards AI safety\cite{FLIAISafetyIndex2025}\cite{AISafetyReport2025}.

% Maybe explain how there is good work being done in grey literature https://www.alignment.org/blog/competing-with-sampling/ and how it's hard to parse for a normal person and how there's a bridge lacking between a normal person and real advancements in AI safety research.

% Knowing the gab between safety progress and AI capabilities is widening I intend with my research to build on work such as: AI Control: Improving Safety Despite Intentional Subversion. My goal is to mitigate or demonstrate the risks of unsafe systems, in the hope that sufficiently robust misalignment demonstrations may instill enough public discourse to warrant and enable further systemic action.

\section{Objectives}

In the world of AI safety, there's a lot of research surrounding AI alignment, preventing misbehavior in a distributional shift, explainable AI, etc. The proposed hypothesis is the following:
\begin{itemize}
	\item \textbf{Hypothesis}: If the current trajectory of AI development is deemed to be unsafe, then, in the present environment, where every big AI company is incentivized almost existentially into investing large swaths of money into this technology, no amount of AI safety research is enough, meaning that to truly guarantee safe and aligned AI systems for humans in the future, changes to the incentive structure that pushes companies in this direction are also required.
\end{itemize}
Therefore, this research will be conducted in a two-step SLR:
\begin{itemize}
	\item \textbf{SLR 1}: Aims at finding out if the current pursuit of AGI has natural dangers, how dangerous, and weather they are relevant in general or only narrow domains.
	\item \textbf{SLR 2}: If danger is found in the current approach, what are the incentives that keep steering and brought about the current situation? What are they sensitive to, and how can they be analyzed and changed?
\end{itemize}

Thus, this first literature review shall focus on work that has been carried out in analyzing the intent and inner objectives of current generalist models, for that purpose, the following research questions will be used to carry out the research on \textbf{SLR 1}:

\begin{enumerate}
	\item RQ1. What is the main focus of current AI safety research?
	\item RQ2. Witch are the current methods used to look into the alignment of current generalist models?
	\item RQ3. What conclusions regarding the safety of current models and safety of development and deployment processes can we assert from current research?
	\item RQ4. What metrics can be used to more clearly show potential problems with these systems?
\end{enumerate}

\section{Dissertation Structure}

\newpage
