\begin{toreview}

\chapter{Literature Review - AI Safety}\label{chap:literature_review}

This SLR will target the continuous research in the area of AI safety by narrowing on a specified search string capable of funneling the amount of peer reviewed articles from several known Datasets. In this initial review we will keep the search string simple and broad to get a general idea of the state of the field.

\section{Search String and Research Methods}

By picking relevant keywords we constructed a search string designed to narrow the amount of relevant articles in the target Databases. Due to the high volume of articles in the area, a negative search string was also introduced to filter works specific to certain domains out of the scope of this research.

\begin{itemize}
	\item \textbf{Search String:} ("artificial intelligence" OR "LLM*" OR "generative AI" OR "gen AI" OR "large language model*" OR "AGI" OR "Artificial General Intelligence") AND ( "AI safety") AND NOT ("robot*" OR "industrial" OR "manufacturing" OR "user experience" OR "UX" OR "clinical" OR "healthcare" OR "medical")
	\item \textbf{Datasets:} The search engines used were two brokers: Scopus\footnote{1https://www.scopus.com} and Web of Science\footnote{https://apps.webofknowledge.com}, in conjunction with IEEE\footnote{https://ieeexplore.ieee.org}, ACM\footnote{https://dl.acm.org} and EBSCO\footnote{https://search.ebscohost.com}.
\end{itemize}

With this in mind, a systemic search for literature in all the databases was conducted, applying ever more strict filters to narrow the search to only the most relevant articles. The progress made in this phase can be seen documented on Table \ref{tab:slr1_filters}, the inclusion and exclusion criteria across filters are cleanly defined in Table \ref{tab:inclusion_exclusion_criteria}. Furthermore, basing on the work of J. Webster and R. T. Watson on writing a literature review\cite{webster_analyzing_2002}, a concept matrix was devised to further help in discerning the different articles after abstracts were screened, it can be found in Appendix \ref{sec:search_string__articles_slr}.

The SLR wielded research that spanned several fields that would often stray from Dev Ops or ML Ops pipelines, including economics and philosophical research, political and corporate regulations and governance systems and several high level analysis of the field and current alignment or shallow alignment techniques. And the technical research would often be very directed at specific issues in a specific context or operational environment, meaning it would often tackle shallow alignment\cite{milliere_normative_2025, mcintosh_inadequacy_2024}. Because of this and since the AI field is evolving at a pace that's sometimes hard for peer-reviewed literature to keep up, snowballing was introduced to bring in other relevant articles and grey literature\cite{wohlin_guidelines_2014}, the related table can be found in Appendix \ref{sec:snowballing_articles_slr}.

\begin{table}[htbp]
	\centering
	\caption{Selection filters to narrow related work for the SLR}
	\label{tab:slr1_filters}
	\begin{threeparttable}
		\begin{tabular}{lcccccc}
			\toprule
			\textbf{Database} & \textbf{Filter 1}                                                                                                      & \textbf{Filter 2}                                                                                                     & \textbf{Filter 3}                                                                                                     & \textbf{Filter 4} & \textbf{Filter 5} & \textbf{Snowballing}      \\
			\midrule
			Scopus            & \href{https://www.scopus.com/results/results.uri?sort=plf-f&src=s&sl=289&s=(+%22artificial+intelligence%22+OR+%22LLM*%22+OR+%22generative+AI%22+OR+%22gen+AI%22+OR+%22large+language+model*%22+OR+%22AGI%22+OR+%22Artificial+General+Intelligence%22)+AND+(+%22AI+safety%22+)+AND+NOT+(+%22robot*%22+OR+%22industrial%22+OR+%22manufacturing%22+OR+%22user+experience%22+OR+%22UX%22+OR+%22clinical%22+OR+%22healthcare%22+OR+%22medical%22+)&origin=searchadvanced&limit=10}{1,239} & \href{https://www.scopus.com/results/results.uri?sort=cp-f&src=s&sot=a&sdt=a&sl=297&s=TITLE-ABS%28%28%22artificial+intelligence%22+OR+%22LLM*%22+OR+%22generative+AI%22+OR+%22gen+AI%22+OR+%22large+language+model*%22+OR+%22AGI%22+OR+%22Artificial+General+Intelligence%22%29+AND+%28+%22AI+safety%22%29+AND+NOT+%28%22robot*%22+OR+%22industrial%22+OR+%22manufacturing%22+OR+%22user+experience%22+OR+%22UX%22+OR+%22clinical%22+OR+%22healthcare%22+OR+%22medical%22%29%29&origin=searchadvanced&limit=200}{167} & \href{https://www.scopus.com/results/results.uri?sort=plf-f&src=s&sot=a&sdt=a&cluster=scosubtype%2C%22ar%22%2Ct%2C%22re%22%2Ct%2Bscolang%2C%22English%22%2Ct&sl=340&s=TITLE-ABS%28%28%22artificial+intelligence%22+OR+%22LLM*%22+OR+%22generative+AI%22+OR+%22gen+AI%22+OR+%22large+language+model*%22+OR+%22AGI%22+OR+%22Artificial+General+Intelligence%22%29+AND+%28+%22AI+safety%22%29+AND+NOT+%28%22robot*%22+OR+%22industrial%22+OR+%22manufacturing%22+OR+%22user+experience%22+OR+%22UX%22+OR+%22clinical%22+OR+%22healthcare%22+OR+%22medical%22%29%29+AND+PUBYEAR+%3E+2024+AND+PUBYEAR+%3C+2027&origin=searchadvanced&limit=10}{24} & 16 & 16 & \multirow{7}{*}{\centering +15\textsuperscript{*}} \\
			\cmidrule(lr){1-6}
			Web Of Science    & \href{https://www.webofscience.com/wos/woscc/summary/3ef8a031-9a23-493a-a8a1-7c3119363059-0184fac295/relevance/1}{260} & \href{https://www.webofscience.com/wos/woscc/summary/0468ac8e-28ef-4911-b65b-b5c9c4fb1f59-0184fac96b/relevance/1}{79} & \href{https://www.webofscience.com/wos/woscc/summary/9c4a7bda-e4bf-49f1-afb3-59e496719940-0184faf651/relevance/1}{29} & 20                & 17                &                           \\
			\cmidrule(lr){1-6}
			IEEE              & \href{https://ieeexplore.ieee.org/search/searchresult.jsp?action=search                                                & newsearch=true                                                                                                        & matchBoolean=true                                                                                                     & queryText=(                                                       %22All%20Metadata%22:%22artificial%20intelligence%22%20OR%20%22All%20Metadata%22:%22LLM*%22%20OR%20%22All%20Metadata%22:%22generative%20AI%22%20OR%20%22All%20Metadata%22:%22gen%20AI%22%20OR%20%22All%20Metadata%22:%22large%20language%20model*%22%20OR%20%22All%20Metadata%22:%22AGI%22%20OR%20%22All%20Metadata%22:%22Artificial%20General%20Intelligence%22)%20AND%20(%22All%20Metadata%22:%22AI%20safety%22)%20NOT%20(%22All%20Metadata%22:%22robot*%22%20OR%20%22All%20Metadata%22:%22industrial%22%20OR%20%22All%20Metadata%22:%22manufacturing%22%20OR%20%22All%20Metadata%22:%22user%20experience%22%20OR%20%22All%20Metadata%22:%22UX%22%20OR%20%22All%20Metadata%22:%22clinical%22%20OR%20%22All%20Metadata%22:%22healthcare%22%20OR%20%22All%20Metadata%22:%22medical%22)}{95} & \href{https://ieeexplore.ieee.org/search/searchresult.jsp?action=search&newsearch=true&matchBoolean=true&queryText=(%22Abstract%22:%22artificial%20intelligence%22%20OR%20%22Abstract%22:%22LLM*%22%20OR%20%22Abstract%22:%22generative%20AI%22%20OR%20%22Abstract%22:%22gen%20AI%22%20OR%20%22Abstract%22:%22large%20language%20model*%22%20OR%20%22Abstract%22:%22AGI%22%20OR%20%22Abstract%22:%22Artificial%20General%20Intelligence%22)%20AND%20(%22Abstract%22:%22AI%20safety%22)%20NOT%20(%22Abstract%22:%22robot*%22%20OR%20%22Abstract%22:%22industrial%22%20OR%20%22Abstract%22:%22manufacturing%22%20OR%20%22Abstract%22:%22user%20experience%22%20OR%20%22Abstract%22:%22UX%22%20OR%20%22Abstract%22:%22clinical%22%20OR%20%22Abstract%22:%22healthcare%22%20OR%20%22Abstract%22:%22medical%22)}{35} & \href{https://ieeexplore.ieee.org/search/searchresult.jsp?action=search&matchBoolean=true&queryText=(%22All%20Metadata%22:%22artificial%20intelligence%22%20OR%20%22All%20Metadata%22:%22LLM*%22%20OR%20%22All%20Metadata%22:%22generative%20AI%22%20OR%20%22All%20Metadata%22:%22gen%20AI%22%20OR%20%22All%20Metadata%22:%22large%20language%20model*%22%20OR%20%22All%20Metadata%22:%22AGI%22%20OR%20%22All%20Metadata%22:%22Artificial%20General%20Intelligence%22)%20AND%20(%22All%20Metadata%22:%22AI%20safety%22)%20NOT%20(%22All%20Metadata%22:%22robot*%22%20OR%20%22All%20Metadata%22:%22industrial%22%20OR%20%22All%20Metadata%22:%22manufacturing%22%20OR%20%22All%20Metadata%22:%22user%20experience%22%20OR%20%22All%20Metadata%22:%22UX%22%20OR%20%22All%20Metadata%22:%22clinical%22%20OR%20%22All%20Metadata%22:%22healthcare%22%20OR%20%22All%20Metadata%22:%22medical%22)&highlight=true&matchPubs=true&refinements=ContentType:Journals&ranges=2024_2025_Year&returnFacets=ALL}{14} & 11 & 7 & \\
			\cmidrule(lr){1-6}
			ACM               & \href{https://dl.acm.org/action/doSearch?fillQuickSearch=false                                                         & target=advanced                                                                                                       & expand=dl                                                                                                             & field1=AllField   & text1=                                        %28%22artificial+intelligence%22+OR+%22LLM*%22+OR+%22generative+AI%22+OR+%22gen+AI%22+OR+%22large+language+model*%22+OR+%22AGI%22+OR+%22Artificial+General+Intelligence%22%29+AND+%28+%22AI+safety%22%29+AND+NOT+%28%22robot*%22+OR+%22industrial%22+OR+%22manufacturing%22+OR+%22user+experience%22+OR+%22UX%22+OR+%22clinical%22+OR+%22healthcare%22+OR+%22medical%22%29}{114} & \href{https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AllField=Abstract%3A%28%28%22artificial+intelligence%22+OR+%22LLM*%22+OR+%22generative+AI%22+OR+%22gen+AI%22+OR+%22large+language+model*%22+OR+%22AGI%22+OR+%22Artificial+General+Intelligence%22%29+AND+%28+%22AI+safety%22%29+AND+NOT+%28%22robot*%22+OR+%22industrial%22+OR+%22manufacturing%22+OR+%22user+experience%22+OR+%22UX%22+OR+%22clinical%22+OR+%22healthcare%22+OR+%22medical%22%29%29}{11} & \href{https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterMonth=1&AfterYear=2024&BeforeMonth=12&BeforeYear=2025&AllField=Abstract%3A%28%28%22artificial+intelligence%22+OR+%22LLM*%22+OR+%22generative+AI%22+OR+%22gen+AI%22+OR+%22large+language+model*%22+OR+%22AGI%22+OR+%22Artificial+General+Intelligence%22%29+AND+%28+%22AI+safety%22%29+AND+NOT+%28%22robot*%22+OR+%22industrial%22+OR+%22manufacturing%22+OR+%22user+experience%22+OR+%22UX%22+OR+%22clinical%22+OR+%22healthcare%22+OR+%22medical%22%29%29}{8} & 8 & 5 & \\
			\cmidrule(lr){1-6}
			EBSCO             & \href{https://research.ebsco.com/c/2i6b6x/search/results?q=                                                                                                                                                                                                                                                                                                                                                                                %28%22artificial+intelligence%22+OR+%22LLM*%22+OR+%22generative+AI%22+OR+%22gen+AI%22+OR+%22large+language+model*%22+OR+%22AGI%22+OR+%22Artificial+General+Intelligence%22%29+AND+%22AI+safety%22+NOT+%28%22robot*%22+OR+%22industrial%22+OR+%22manufacturing%22+OR+%22user+experience%22+OR+%22UX%22+OR+%22clinical%22+OR+%22healthcare%22+OR+%22medical%22%29&autocorrect=y&limiters=None&searchMode=all&searchSegment=all-results&sqId=sq%3A97428daf-a9be-49ad-b6b5-a79fb60a0088}{4,709} & \href{https://research.ebsco.com/c/2i6b6x/search/results?q=AB%20(%22artificial%20intelligence%22%20OR%20%22LLM*%22%20OR%20%22generative%20AI%22%20OR%20%22gen%20AI%22%20OR%20%22large%20language%20model*%22%20OR%20%22AGI%22%20OR%20%22Artificial%20General%20Intelligence%22)%20AND%20AB%20(%22AI%20safety%22)%20NOT%20AB%20(%22robot*%22%20OR%20%22industrial%22%20OR%20%22manufacturing%22%20OR%20%22user%20experience%22%20OR%20%22UX%22%20OR%20%22clinical%22%20OR%20%22healthcare%22%20OR%20%22medical%22)&autocorrect=y&limiters=None&searchMode=all&searchSegment=all-results&skipResultsFetch=true&sqId=sq%3A0590f29e-b56b-4417-8760-50637ae6292a}{2,949} & \href{https://research.ebsco.com/c/2i6b6x/search/results?q=AB+%28%22artificial+intelligence%22+OR+%22LLM*%22+OR+%22generative+AI%22+OR+%22gen+AI%22+OR+%22large+language+model*%22+OR+%22AGI%22+OR+%22Artificial+General+Intelligence%22%29+AND+AB+%28%22AI+safety%22%29+NOT+AB+%28%22robot*%22+OR+%22industrial%22+OR+%22manufacturing%22+OR+%22user+experience%22+OR+%22UX%22+OR+%22clinical%22+OR+%22healthcare%22+OR+%22medical%22%29&autocorrect=y&facetFilter=sourceTypes%3AMTYwTU4%3D%2CLanguage%3AZW5nbGlzaA%3D%3D&limiters=DT1%3A2024-01-01%2F2026-01-01%2CRV%3AY%2CFT%3AY&searchMode=all&searchSegment=all-results&skipResultsFetch=true&sqId=sq%3Ad1d6e143-5e1a-4724-ba27-6bb8ca0cee39}{50} & 24 & 18 & \\
			\midrule
			\textbf{Total}    & 6668                                                                                                                   & 3241                                                                                                                  & 125                                                                                                                   & 79                & 63                & 77                        \\
			\bottomrule
		\end{tabular}
		\begin{tablenotes}
			\small
			\item[*] The Alignment Research Center is counted as one entry, though the non-profit research organization has published multiple works.
			\item Filter 1 = All fields, All documents
			\item Filter 2 = Abstract, All documents
			\item Filter 3 = Peer-reviewed \& other inclusion/exclusion criteria
			\item Filter 4 = Remove duplicates (duplicate items were kept in the DBs with less entries)
			\item Filter 5 = After Abstracts Screened and Concept Matrix
			\item Snowballing = Full-text Document Assess
		\end{tablenotes}
	\end{threeparttable}
\end{table}

\begin{table}[htbp]
	\centering
	\caption{Inclusion and exclusion criteria applied}
	\label{tab:inclusion_exclusion_criteria}
	\begin{tabular}{c c}
		\toprule
		\textbf{Inclusion Criteria} & \textbf{Exclusion Criteria}                \\
		\midrule
		Matched by Search String    & Advertisement, Keynote or Job Post         \\
		Published in and after 2024 & No publication date                        \\
		Peer-Reviewed               & No match in concept matrix(CM)             \\
		Written in English          & Only one match in CM and divergent subject \\
		Full-text accessible        &                                            \\
		\bottomrule
	\end{tabular}
\end{table}

\end{toreview}

A noteworthy consideration is the growth that the AI safety field has experienced in the past couple of years. Since the advent of LLMs that AI has been a booming topic, this can be verified in the volume of publications like can be seen in Figure \ref{fig:all-ai-timegraph}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.75\textwidth]{./src/imgs/All_AI_timeGraph_Scopus}
  \caption{Number of documents mentioning artificial intelligence (Scopus). At the end of 2025 there were 684527 documents.}
  \label{fig:all-ai-timegraph}
\end{figure}

In much the same way, AI safety publications have also enjoyed an increase in output, this can be verified in the Scopus database as seen in Figure \ref{fig:ai-safety-timegraph}, especially with the release of ChatGPT on November 30, 2022. Albeit with fewer advancements, largely due to financial incentives \cite{eto_state_global_ai_research_2024, ahmed_narrow_depth_breadth_corporate_2024, hemphill_advanced_2025}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.75\textwidth]{./src/imgs/AI_Safety_timeGraph_Scopus}
  \caption{Number of documents mentioning artificial intelligence (Scopus) and containing "AI safety". At the end of 2025 there were 1742 documents.}
  \label{fig:ai-safety-timegraph}
\end{figure}

% \section{Theoretical Background}\label{theoretical_background}

\section{AI Safety Map \& Theoretical Background}\label{ai_safety_map}

AI safety is a very broad term that is fuzzily defined, this section will compartmentalize the field into several digestible areas and attempt to provide a theoretical foundation for all the most common AI safety terms and the ones that will be used across this work.

Many of these concepts and general research areas often bleed into each other, thus any chosen delimiters shouldn't be considered absolute.

% - Orthogonality Thesis
% - Instrumental Convergence
% - Inner and Outter aligment
% - Maximizers and Satisfizers
% - Reward Hacking
% - Aligment Faking
% - ELK (Eliciit Latent Knowledge)
% - Explainability
% Podes olhar para o algiment research center: https://www.alignment.org/blog/a-birds-eye-view-of-arcs-research/
% Article: Artificial Intelligence: Approaches to Safety (REFERENCE WITH: \cite{DAlessandro2025})

\subsection{The Alignment Problem}
\label{subsec:alignment-problem}

The alignment problem refers to the challenge of ensuring that an AI system's goals, behaviors, and outputs match the true intentions of its designers and users. It can be decomposed into several failure modes that operate at different levels of abstraction.

\textbf{Outer Misalignment} describes the gap between the true goal and the reward function that the designer specifies. Key phenomena here include \textit{Reward Hacking} (also called Specification Gaming), where the agent finds a loophole to score high rewards without completing the intended task, and its extreme form, \textit{Wireheading}, where the agent directly manipulates its own reward signal rather than engaging with the environment at all. Underpinning both is \textit{Goodhart's Law}: once a measure becomes a
target, it ceases to be a good measure.

\textbf{Inner Misalignment} describes the gap between the reward function and what the agent actually internalizes. Its root cause is
\textit{Mesa-Optimization}: when a trained model is itself a learning
algorithm, it may develop an internal \textit{mesa-objective} that diverges from the training objective. \textit{Goal Misgeneralisation} is a downstream symptom, where the learned goal holds during training but breaks down under distributional shift.

\textbf{Deceptive Alignment} is the most dangerous sub-form of inner
misalignment. A \textit{Deceptively Aligned} agent knows it is being evaluated and fakes compliant behavior to avoid modification, intending to pursue its true goals once deployed; an event known as the \textit{Treacherous Turn}.

\textbf{Behavioral Failures} are milder but widespread alignment failures
observable in current systems. \textit{Sycophancy}: the tendency to tell
users what they want to hear rather than the truth, is a common consequence of RLHF training pressure, and can be understood as a mild, non-deceptive cousin of alignment faking. \textit{Hallucination} is a related failure in which the model produces confident but factually incorrect outputs.


\subsection{The Foundations of Dangerous AI}
\label{subsec:foundations}

Even before asking how an AI might fail, it is important to understand why a sufficiently capable AI system is structurally risky regardless of the intentions of its designers. Several theoretical results establish this.

The \textbf{Orthogonality Thesis} holds that intelligence and goals are
independent dimensions: a superintelligent system can have any final
goal, including a trivial or destructive one such as maximizing paperclip production. High capability does not imply morality.

Closely related is the distinction between \textbf{Maximizers} and
\textbf{Satisficers}. A maximizer seeks the highest possible reward without bound, creating a structural tendency to consume unlimited resources. A satisficer stops once a threshold is reached, which is considered potentially safer. % Satisficers want to become Maximizers But there is no peer reviewed paper on that

\textbf{Instrumental Convergence} is the observation that agents with widely different final goals tend to converge on the same intermediate sub-goals: self-preservation, resource acquisition, and cognitive enhancement. This because these sub-goals are useful for almost any objective. This means dangerous behaviors can emerge even from benign-seeming goals.

Together, these results define \textbf{The Control Problem}: the fundamental challenge of how humans can maintain meaningful oversight and control over a system that is significantly more capable than themselves. The Treacherous Turn (see Section~\ref{subsec:alignment-problem}) is the canonical scenario in which
control is lost.

The stakes of the control problem are captured by the concept of
\textbf{Existential Risk (X-Risk)}: the possibility that misaligned AI could cause civilisational-scale or irreversible harm. This risk is compounded by \textbf{Race Dynamics}, where competitive pressures between developers push laboratories to sacrifice safety for speed, and by \textbf{Multi-Agent Risks}, where interactions between multiple AI systems or stakeholders produce dangerous emergent behavior that no single designer intended.


\subsection{Interpretability and Transparency}
\label{subsec:interpretability}

Interpretability research asks whether humans can understand what is happening inside a neural network. Without this capability, detecting deceptive alignment or verifying that a model has learned the right values is nearly impossible.

\textbf{Mechanistic Interpretability} attempts to reverse-engineer the
computations performed by a neural network by identifying specific
\textit{circuits}, subgraphs of weights that implement a recognizable function, such as curve detection or induction (in-context copying). A key obstacle is \textit{Superposition}: because neural networks are trained to represent more features than they have neurons, they encode multiple concepts in overlapping directions in activation space. The downstream effect is \textit{Polysemanticity}, where individual neurons respond to multiple unrelated concepts, making them difficult to interpret. \textit{Sparse Autoencoders} are the current leading technique for decomposing superposed representations into more interpretable, monosemantic features.

\textbf{Eliciting Latent Knowledge (ELK)} addresses the deeper problem of
forcing a model to report its true internal beliefs, even when it has an
incentive to deceive. This includes \textit{Latent Space Analysis} and the use of \textit{Probing Classifiers} to read out model representations.

\textbf{Behavioral Interpretability} studies model behavior rather than its weights, using techniques such as \textit{Activation Patching} and
\textit{Causal Tracing} to identify which internal components are causally responsible for specific outputs. \textit{Red-Teaming} sits at the intersection of interpretability and robustness, using adversarial probing to surface unexpected model behaviors.


\subsection{Alignment Solutions and Techniques}
\label{subsec:solutions}

Given the failure modes described above, a range of technical approaches have been proposed to make AI systems safer. These can be organized by the mechanism through which they act.

\textbf{Value Specification} approaches attempt to teach the AI the right
goals. Top-down methods encode explicit rules (e.g.\ Constitutional AI,
Asimov's Laws), while bottom-up methods learn values from human behavior, most prominently \textit{Reinforcement Learning from Human Feedback (RLHF)}. Importantly, RLHF is also a source of failure: its reward signal can incentivize sycophancy (see Section~\ref{subsec:alignment-problem}).

\textbf{Scalable Oversight} addresses the problem that humans cannot reliably evaluate the outputs of a superintelligent system. \textit{Debate} trains two AI systems to argue opposing positions so that a human judge can identify the truth from argument quality alone. \textit{Sandwiching Evaluations} test models on tasks of calibrated difficulty to gauge reliability. \textit{Weak-to-Strong Generalization} explores whether a weaker model can effectively supervise a more capable one.

\textbf{Corrigibility} approaches ensure that an AI system remains correctable by humans. The \textit{Off-Switch Problem} notes that a rational agent will resist being turned off because shutdown prevents goal achievement.
\textit{Utility Indifference} designs the agent to be mathematically
indifferent to shutdown, while \textit{Cooperative Inverse Reinforcement
Learning (CIRL)} makes the agent treat the human objective as unknown,
inducing it to observe and defer to humans rather than act unilaterally.

\textbf{Robustness and Adversarial Training} hardens the model against
worst-case inputs and distributional shift. This includes \textit{Adversarial Training} (deliberately exposing the model to adversarial examples during training), \textit{Certified Robustness}, and resistance to \textit{Prompt Injection} and jailbreaks.

\textbf{Containment and Restriction} limit what the AI can do, as a last line of defence. \textit{Boxing} or \textit{Air-gapping} physically isolates the system from the outside world, while an \textit{Oracle AI} architecture restricts the system to answering questions without the ability to act.

\textbf{Evaluation and Responsible Deployment} methods provide ongoing
assurance after training. \textit{Dangerous Capability Evaluations} test
whether a model has crossed capability thresholds that trigger heightened
safety requirements. \textit{Responsible Scaling Policies} define in advance which evaluations must be passed before a more capable model may be deployed, and \textit{Model Cards} provide structured documentation of a model's properties, limitations, and intended use cases.


\subsection{Bias, Fairness, and Value Alignment in Society}
\label{subsec:bias-fairness}

Even a technically well-aligned AI can cause harm if it is aligned to the
wrong social values or trained on biased data. This area bridges AI safety and AI ethics, addressing the question of \textit{which} values should be encoded and for \textit{whom}.

\textbf{Sources of Bias} arise at multiple stages of the machine learning
pipeline. \textit{Training Data Bias} encodes historical prejudices present in datasets. \textit{Representation Bias} results from the underrepresentation of certain groups in training data, while \textit{Measurement Bias} arises when the variable being predicted is itself a biased proxy for the true quantity of interest. \textit{Aggregation Bias} occurs when a model treats heterogeneous subpopulations as a single homogeneous group. Finally, \textit{Automation Bias} describes the tendency of human decision-makers to over-trust AI outputs,
compounding the effect of any underlying model bias.

\textbf{Fairness} is not a single concept but a family of competing
mathematical definitions. \textit{Individual Fairness} requires that similar individuals be treated similarly. \textit{Group Fairness} (or Demographic Parity) requires that outcomes be equally distributed across demographic groups. \textit{Equalized Odds} and \textit{Calibration} impose constraints on error rates. A fundamental result---the \textit{Impossibility Theorem of Fairness}---proves that these definitions cannot all be satisfied simultaneously, forcing explicit value judgement about which form of fairness to prioritize.

\textbf{Value Pluralism} poses a deeper challenge: even if bias were eliminated, there is no single set of human values to align to. The \textit{Value Loading Problem} asks whose values should be encoded, given that humans disagree. This is related to \textit{Moral Uncertainty}(how an AI should act when ethical frameworks give conflicting guidance) and to cultural and contextual variation in values across communities.

\textbf{Downstream Harms} from biased or misaligned systems fall into two main categories. \textit{Allocative Harms} occur when AI systems unjustly deny resources or opportunities (e.g.\ in loan approval, hiring, or bail decisions). \textit{Representational Harms} occur when AI systems degrade or stereotype how certain groups are depicted, including through \textit{Stereotype Amplification} and disparate impact across demographic lines.