# Dissertation-AI-safety
Dissertation On AI safety

Disclaimer: Some nix code, and Github actions code was done with the help of LLMs like Claude.ai

## Submitted title and theme as of 30th of November 2025:

**Title**: AI Safety and Alignment in Real-World Contexts

**Theme**:

The dangers of unaligned AI frequently start with Bostrom’s “Paperclip Maximizer” which demonstrates how an AI system with seemingly benign goals could still potentially lead to catastrophic outcomes if not properly aligned with human values. This thought experiment is rooted in the orthogonality thesis witch states that there is no inherent link between a system's goals and its ability to reach them.
Work form Yampolskiy [On the Controllability of Artificial Intelligence: An Analysis of Limitations] argues that it's definitionally impossible for humans to understand or control a super-intelligent agent. He appeals for, and signed, the Future of Life Institute Open letter (https://superintelligence-statement.org/) calling for a cease to the development of super-intelligent AI until there is:
1. broad scientific consensus that it will be done safely and controllably, and
2. strong public buy-in.

It's hard for humans to grasp the severity of the problem due cognitive Bias [Human biases and remedies in AI safety and alignment contexts] and structural economic and global systems that incentivize companies to invest in increasingly capable AIs [Advanced artificial intelligence at a corporate responsibility crossroads: employees as risk management advocates] often while disregarding safety considerations [AI Safety Index - Summer 2025 Edition].

Today we have Multi-modal and Generalist LLMs capable of doing tasks in a large range of domains that exhibit early signs of instrumental convergence behavior [Alignment faking in large language models].

Knowing the gab between safety progress and AI capabilities is widening I intend with my research to build on work such as: AI Control: Improving Safety Despite Intentional Subversion. My goal is to mitigate or demonstrate the risks of unsafe systems, in the hope that sufficiently robust misalignment demonstrations may instill enough public discourse to warrant and enable further systemic action.

## Code & Latex

### Color Coding
- Text in <span style="color:red">**Red**</span> still needs to be reviewed. Set it with:
    ```tex
    \begin{toreview}
    ...
    \end{toreview}
    ```
- Text in <span style="color:green">**Green**</span> is already reviewed. Set it with:
    ```tex
    \begin{reviewed}
    ...
    \end{reviewed}
    ```
- Text in **Black** is still being worked on.